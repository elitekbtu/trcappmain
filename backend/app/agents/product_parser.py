#!/usr/bin/env python3
"""
Modern Lamoda Product Parser

–ü–∞—Ä—Å–µ—Ä –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –ø–æ URL, –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–∑ —Å—Ç–∞—Ä–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞
–Ω–æ —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º–∏ –∏ –º–µ—Ç–æ–¥–∞–º–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö.
"""

import asyncio
import json
import re
from dataclasses import dataclass
from typing import List, Optional, Dict, Any
from urllib.parse import urljoin, urlparse

import httpx
from bs4 import BeautifulSoup

from .parser_agent import Product


@dataclass
class ProductDetails:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ —Ç–æ–≤–∞—Ä–µ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø–æ–ª—è–º–∏"""
    sku: str
    name: str
    brand: str
    price: float
    old_price: Optional[float]
    url: str
    image_url: str
    image_urls: List[str]
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
    description: Optional[str] = None
    type: Optional[str] = None
    color: Optional[str] = None
    sizes: List[str] = None
    rating: Optional[float] = None
    reviews_count: Optional[int] = None


class ModernLamodaParser:
    """–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —Ç–æ–≤–∞—Ä–æ–≤ Lamoda –ø–æ URL"""
    
    def __init__(self, domain: str = "kz"):
        self.domain = domain
        self.base_url = f"https://www.lamoda.{domain}"
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        self.session = None

    async def _get_session(self):
        """–ü–æ–ª—É—á–∏—Ç—å –∏–ª–∏ —Å–æ–∑–¥–∞—Ç—å HTTP —Å–µ—Å—Å–∏—é"""
        if self.session is None:
            timeout = httpx.Timeout(30.0)
            self.session = httpx.AsyncClient(
                headers=self.headers,
                timeout=timeout,
                follow_redirects=True
            )
        return self.session

    async def parse_product_by_url(self, url: str) -> Optional[ProductDetails]:
        """–ü–∞—Ä—Å–∏—Ç —Ç–æ–≤–∞—Ä Lamoda –ø–æ URL"""
        try:
            print(f"üîç Parsing product: {url}")
            
            session = await self._get_session()
            response = await session.get(url)
            
            if response.status_code != 200:
                print(f"‚ùå HTTP {response.status_code} for {url}")
                return None
            
            print(f"‚úÖ Successfully fetched page (length: {len(response.text)})")
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
            product = self._parse_from_json(soup, url) or self._parse_from_html(soup, url)
            
            if product:
                print(f"‚úÖ Successfully parsed: {product.name} by {product.brand}")
                return product
            else:
                print(f"‚ùå Failed to parse product from {url}")
                return None
                
        except Exception as e:
            print(f"‚ùå Error parsing {url}: {e}")
            return None

    def _parse_from_json(self, soup: BeautifulSoup, url: str) -> Optional[ProductDetails]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —Ç–æ–≤–∞—Ä–∞ –∏–∑ JSON –≤ —Å–∫—Ä–∏–ø—Ç–∞—Ö"""
        try:
            scripts = soup.find_all('script')
            
            for script in scripts:
                if not script.string:
                    continue
                    
                content = script.string.strip()
                
                # –ò—â–µ–º JSON-LD —Å—Ç—Ä—É–∫—Ç—É—Ä—É (schema.org)
                if '"@type": "Product"' in content:
                    product = self._parse_json_ld_product(content, url)
                    if product:
                        return product
                
                # –ò—â–µ–º __NUXT__ –¥–∞–Ω–Ω—ã–µ
                if 'var __NUXT__' in content:
                    product = self._parse_nuxt_data(content, url)
                    if product:
                        return product
            
            return None
            
        except Exception as e:
            print(f"‚ùå Error parsing JSON: {e}")
            return None

    def _parse_json_ld_product(self, content: str, url: str) -> Optional[ProductDetails]:
        """–ü–∞—Ä—Å–∏—Ç —Ç–æ–≤–∞—Ä –∏–∑ JSON-LD —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        try:
            # –ò—â–µ–º JSON –æ–±—ä–µ–∫—Ç —Å –ø—Ä–æ–¥—É–∫—Ç–æ–º
            json_match = re.search(r'\[?\{[^}]*"@type":\s*"Product"[^}]*\}[^}]*\}?\]?', content)
            if not json_match:
                return None
            
            # –û—á–∏—â–∞–µ–º –æ—Ç HTML entities
            json_str = json_match.group(0)
            json_str = json_str.replace('&quot;', '"').replace('&amp;', '&')
            
            # –£–±–∏—Ä–∞–µ–º –º–∞—Å—Å–∏–≤ –µ—Å–ª–∏ –µ—Å—Ç—å
            if json_str.startswith('[') and json_str.endswith(']'):
                json_str = json_str[1:-1]
            
            data = json.loads(json_str)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            brand = "Unknown"
            if 'brand' in data and isinstance(data['brand'], dict):
                brand = data['brand'].get('name', 'Unknown').replace('"', '')
            
            name = data.get('name', 'Unknown Product').replace('"', '')
            
            # –¶–µ–Ω–∞
            price = 0.0
            old_price = None
            
            if 'offers' in data and isinstance(data['offers'], dict):
                offers = data['offers']
                if 'price' in offers:
                    try:
                        price = float(offers['price'])
                    except (ValueError, TypeError):
                        pass
            
            # SKU
            sku = data.get('sku', self._generate_sku_from_url(url))
            
            # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–ø–æ–∫–∞ –ø—É—Å—Ç—ã–µ, –±—É–¥–µ–º –∏—Å–∫–∞—Ç—å –≤ HTML)
            image_urls = []
            
            return ProductDetails(
                sku=sku,
                name=name,
                brand=brand,
                price=price,
                old_price=old_price,
                url=url,
                image_url="",
                image_urls=image_urls,
                description=data.get('description'),
                type=self._extract_type_from_name(name)
            )
            
        except Exception as e:
            print(f"‚ùå Error parsing JSON-LD: {e}")
            return None

    def _parse_nuxt_data(self, content: str, url: str) -> Optional[ProductDetails]:
        """–ü–∞—Ä—Å–∏—Ç —Ç–æ–≤–∞—Ä –∏–∑ NUXT –¥–∞–Ω–Ω—ã—Ö"""
        try:
            # –ò—â–µ–º payload –≤ NUXT –¥–∞–Ω–Ω—ã—Ö
            nuxt_match = re.search(r'var __NUXT__\s*=\s*({.*?});', content, re.DOTALL)
            if not nuxt_match:
                return None
            
            # –ü–∞—Ä—Å–∏–º JSON
            nuxt_data = json.loads(nuxt_match.group(1))
            
            # –ò—â–µ–º –¥–∞–Ω–Ω—ã–µ –æ —Ç–æ–≤–∞—Ä–µ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö
            payload = nuxt_data.get('payload', {})
            
            # –ú–æ–∂–µ–º –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑ NUXT –¥–∞–Ω–Ω—ã—Ö
            # –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None, —á—Ç–æ–±—ã –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å HTML –ø–∞—Ä—Å–∏–Ω–≥
            return None
            
        except Exception as e:
            print(f"‚ùå Error parsing NUXT data: {e}")
            return None

    def _parse_from_html(self, soup: BeautifulSoup, url: str) -> Optional[ProductDetails]:
        """–ü–∞—Ä—Å–∏—Ç —Ç–æ–≤–∞—Ä –∏–∑ HTML —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º–∏"""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∏ –±—Ä–µ–Ω–¥ –∏–∑ h1
            h1_tag = soup.find('h1')
            if not h1_tag:
                return None
            
            h1_text = h1_tag.get_text(strip=True)
            print(f"Found h1: {h1_text}")
            
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ –±—Ä–µ–Ω–¥ –∏ –Ω–∞–∑–≤–∞–Ω–∏–µ
            brand = "Unknown"
            name = h1_text
            
            # –ï—Å–ª–∏ –≤ h1 –µ—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫, –ø–µ—Ä–≤–∞—è –æ–±—ã—á–Ω–æ –±—Ä–µ–Ω–¥
            h1_lines = [line.strip() for line in h1_text.split('\n') if line.strip()]
            if len(h1_lines) >= 2:
                brand = h1_lines[0]
                name = ' '.join(h1_lines[1:])
            else:
                # –ï—Å–ª–∏ –æ–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞, –ø—ã—Ç–∞–µ–º—Å—è —Ä–∞–∑–¥–µ–ª–∏—Ç—å –ø–æ –∏–∑–≤–µ—Å—Ç–Ω—ã–º –±—Ä–µ–Ω–¥–∞–º
                known_brands = ['Nike', 'Adidas', 'Puma', 'Reebok', 'Jordan', 'Converse', 
                               'New Balance', 'Vans', 'Under Armour', 'Asics', 'Mizuno',
                               'Skechers', 'Fila', 'Kappa', 'Umbro', 'Diadora', 'Calvin Klein',
                               'Tommy Hilfiger', 'Lacoste', 'Polo Ralph Lauren', 'Hugo Boss']
                
                for brand_name in known_brands:
                    if h1_text.startswith(brand_name):
                        brand = brand_name
                        name = h1_text[len(brand_name):].strip()
                        break
            
            # –£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–µ–Ω—ã –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ç–æ–≤–∞—Ä–∞
            price = 0.0
            old_price = None
            
            # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Ü–µ–Ω—ã –≤ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–∞—Ö
            price_info = self._extract_detailed_prices(soup)
            if price_info:
                price = price_info.get('current_price', 0.0)
                old_price = price_info.get('old_price')
            
            # –ù–∞–π–¥–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            image_urls = []
            
            all_imgs = soup.find_all('img')
            for img in all_imgs:
                src = img.get('src') or img.get('data-src')
                if src and 'lmcdn.ru' in src:
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º URL
                    if src.startswith('//'):
                        full_url = 'https:' + src
                    elif src.startswith('/'):
                        full_url = 'https://a.lmcdn.ru' + src
                    else:
                        full_url = src
                    
                    if full_url not in image_urls:
                        image_urls.append(full_url)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º SKU
            sku = self._generate_sku_from_url(url)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–∏–ø —Ç–æ–≤–∞—Ä–∞
            product_type = self._extract_type_from_name(name)
            
            return ProductDetails(
                sku=sku,
                name=name,
                brand=brand,
                price=price,
                old_price=old_price,
                url=url,
                image_url=image_urls[0] if image_urls else "",
                image_urls=image_urls,
                type=product_type
            )
            
        except Exception as e:
            print(f"‚ùå Error parsing HTML: {e}")
            return None

    def _extract_detailed_prices(self, soup: BeautifulSoup) -> Optional[Dict[str, Optional[float]]]:
        """–î–µ—Ç–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–µ–Ω –∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ç–æ–≤–∞—Ä–∞"""
        try:
            price_info = {
                'current_price': None,
                'old_price': None
            }
            
            # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ç–æ–≤–∞—Ä–∞ (–Ω–µ –∫–∞—Ç–∞–ª–æ–≥–∞)
            price_selectors = {
                'current': [
                    '[data-testid="price-current"]',
                    '.price-current',
                    '.price__current',
                    '.product-price__current',
                    'span[class*="price"][class*="current"]',
                    'div[class*="price"][class*="current"]'
                ],
                'old': [
                    '[data-testid="price-old"]',
                    '.price-old',
                    '.price__old',
                    '.product-price__old',
                    'span[class*="price"][class*="old"]',
                    'div[class*="price"][class*="old"]'
                ],
                'single': [
                    '[data-testid="price"]',
                    '.price',
                    '.product-price',
                    'span[class*="price"]:not([class*="old"])',
                    'div[class*="price"]:not([class*="old"])'
                ]
            }
            
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –∞–∫—Ç—É–∞–ª—å–Ω—É—é —Ü–µ–Ω—É
            for selector in price_selectors['current']:
                price_elem = soup.select_one(selector)
                if price_elem:
                    price_text = price_elem.get_text(strip=True)
                    if price_text and ('‚Ç∏' in price_text or '‚ÇΩ' in price_text):
                        price = self._extract_price_from_text(price_text)
                        if price:
                            price_info['current_price'] = price
                            break
            
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Å—Ç–∞—Ä—É—é —Ü–µ–Ω—É
            for selector in price_selectors['old']:
                old_price_elem = soup.select_one(selector)
                if old_price_elem:
                    old_price_text = old_price_elem.get_text(strip=True)
                    if old_price_text and ('‚Ç∏' in old_price_text or '‚ÇΩ' in old_price_text):
                        old_price = self._extract_price_from_text(old_price_text)
                        if old_price:
                            price_info['old_price'] = old_price
                            break
            
            # –ï—Å–ª–∏ –∞–∫—Ç—É–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—â–µ–º –µ–¥–∏–Ω—É—é —Ü–µ–Ω—É
            if not price_info['current_price']:
                for selector in price_selectors['single']:
                    price_elem = soup.select_one(selector)
                    if price_elem:
                        price_text = price_elem.get_text(strip=True)
                        if price_text and ('‚Ç∏' in price_text or '‚ÇΩ' in price_text):
                            price = self._extract_price_from_text(price_text)
                            if price:
                                price_info['current_price'] = price
                                break
            
            # Fallback: –ø–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º—É —Ç–µ–∫—Å—Ç—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            if not price_info['current_price']:
                page_text = soup.get_text()
                price_matches = re.findall(r'(\d{1,3}(?:\s+\d{3})*)\s*[‚Ç∏‚ÇΩ]', page_text)
                
                if price_matches:
                    prices = []
                    for price_str in price_matches:
                        try:
                            p = float(price_str.replace(' ', ''))
                            if 100 <= p <= 10000000:
                                prices.append(p)
                        except ValueError:
                            continue
                    
                    if prices:
                        prices.sort()
                        price_info['current_price'] = prices[0]
                        if len(prices) > 1:
                            # –ë–µ—Ä–µ–º –≤—Ç–æ—Ä—É—é —Ü–µ–Ω—É –∫–∞–∫ —Å—Ç–∞—Ä—É—é, –µ—Å–ª–∏ –æ–Ω–∞ –±–æ–ª—å—à–µ
                            potential_old_price = prices[1]
                            if potential_old_price > prices[0]:
                                price_info['old_price'] = potential_old_price
            
            return price_info if price_info['current_price'] else None
            
        except Exception as e:
            print(f"‚ùå Error extracting detailed prices: {e}")
            return None

    def _extract_price_from_text(self, text: str) -> Optional[float]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–µ–Ω—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞ (–∞–Ω–∞–ª–æ–≥ –º–µ—Ç–æ–¥–∞ –∏–∑ parser_agent)"""
        if not text:
            return None
        
        # –û—á–∏—â–∞–µ–º –æ—Ç HTML —Ç–µ–≥–æ–≤ –∏ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        text = re.sub(r'<[^>]+>', '', text)
        text = text.strip()
        
        # –£–±–∏—Ä–∞–µ–º –≤–∞–ª—é—Ç–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        clean_text = text.replace('‚Ç∏', '').replace('‚ÇΩ', '').replace('—Ä.', '').strip()
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è —Ü–µ–Ω —Å –ø—Ä–æ–±–µ–ª–∞–º–∏
        price_pattern = r'\b(\d{1,3}(?:\s+\d{3})*)\b'
        matches = re.findall(price_pattern, clean_text)
        
        if matches:
            price_str = matches[0].replace(' ', '')
            try:
                price = float(price_str)
                if 100 <= price <= 10000000:
                    return price
            except ValueError:
                pass
        
        # Fallback: —á–∏—Å–ª–∞ –±–µ–∑ –ø—Ä–æ–±–µ–ª–æ–≤
        fallback_pattern = r'\b(\d{3,7})\b'
        fallback_matches = re.findall(fallback_pattern, clean_text)
        
        if fallback_matches:
            for match in fallback_matches:
                try:
                    price = float(match)
                    if 100 <= price <= 10000000:
                        return price
                except ValueError:
                    continue
        
        return None

    def _generate_sku_from_url(self, url: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç SKU –∏–∑ URL"""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–¥ —Ç–æ–≤–∞—Ä–∞ –∏–∑ URL
            # –§–æ—Ä–º–∞—Ç: https://www.lamoda.kz/p/mp002xw0zg9n/clothes-terranova-bryuki/
            # –ù—É–∂–µ–Ω –∞—Ä—Ç–∏–∫—É–ª: mp002xw0zg9n
            path_parts = urlparse(url).path.strip('/').split('/')
            
            # –ò—â–µ–º —á–∞—Å—Ç—å –ø–æ—Å–ª–µ /p/
            if len(path_parts) >= 2 and path_parts[0] == 'p':
                article_code = path_parts[1]
                if len(article_code) >= 8 and article_code.replace('-', '').isalnum():
                    return article_code.upper()
            
            # Fallback - –∏—â–µ–º –ª—é–±—É—é –¥–ª–∏–Ω–Ω—É—é –∞–ª—Ñ–∞–≤–∏—Ç–Ω–æ-—Ü–∏—Ñ—Ä–æ–≤—É—é —á–∞—Å—Ç—å
            for part in path_parts:
                if len(part) >= 8 and part.replace('-', '').replace('_', '').isalnum():
                    return part.upper()
            
            # –ü–æ—Å–ª–µ–¥–Ω–∏–π fallback
            return f"PARSE{hash(url) % 100000:05d}"
            
        except Exception:
            return f"UNKNOWN{hash(url) % 100000:05d}"

    def _extract_type_from_name(self, name: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–∏–ø —Ç–æ–≤–∞—Ä–∞ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"""
        name_lower = name.lower()
        
        # –°–ª–æ–≤–∞—Ä—å —Ç–∏–ø–æ–≤ —Ç–æ–≤–∞—Ä–æ–≤
        type_keywords = {
            '—à–æ—Ä—Ç—ã': '–®–æ—Ä—Ç—ã',
            '–∫—Ä–æ—Å—Å–æ–≤–∫–∏': '–ö—Ä–æ—Å—Å–æ–≤–∫–∏', 
            '—Ñ—É—Ç–±–æ–ª–∫–∞': '–§—É—Ç–±–æ–ª–∫–∞',
            '–ø–ª–∞—Ç—å–µ': '–ü–ª–∞—Ç—å–µ',
            '–±—Ä—é–∫–∏': '–ë—Ä—é–∫–∏',
            '–¥–∂–∏–Ω—Å—ã': '–î–∂–∏–Ω—Å—ã',
            '–∫—É—Ä—Ç–∫–∞': '–ö—É—Ä—Ç–∫–∞',
            '—Å–≤–∏—Ç–µ—Ä': '–°–≤–∏—Ç–µ—Ä',
            '—Ä—É–±–∞—à–∫–∞': '–†—É–±–∞—à–∫–∞',
            '—é–±–∫–∞': '–Æ–±–∫–∞',
            '—Å–∞–±–æ': '–°–∞–±–æ',
            '–∫–µ–¥—ã': '–ö–µ–¥—ã',
            '–±–æ—Ç–∏–Ω–∫–∏': '–ë–æ—Ç–∏–Ω–∫–∏',
            '—Å–∞–ø–æ–≥–∏': '–°–∞–ø–æ–≥–∏'
        }
        
        for keyword, type_name in type_keywords.items():
            if keyword in name_lower:
                return type_name
        
        return "–¢–æ–≤–∞—Ä"

    def to_product(self, product_details: ProductDetails) -> Product:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç ProductDetails –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π Product"""
        return Product(
            sku=product_details.sku,
            name=product_details.name,
            brand=product_details.brand,
            price=product_details.price,
            old_price=product_details.old_price,
            url=product_details.url,
            image_url=product_details.image_url,
            image_urls=product_details.image_urls
        )

    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ HTTP —Å–µ—Å—Å–∏–∏"""
        if self.session:
            await self.session.aclose()
            self.session = None


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
async def test_product_parser():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä—Å–µ—Ä–∞ —Ç–æ–≤–∞—Ä–æ–≤"""
    parser = ModernLamodaParser(domain="kz")
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ URL
    test_urls = [
        "https://www.lamoda.kz/p/rtlaek537801/",  # Nike —à–æ—Ä—Ç—ã
    ]
    
    for url in test_urls:
        product = await parser.parse_product_by_url(url)
        if product:
            print(f"\n‚úÖ SUCCESS:")
            print(f"   SKU: {product.sku}")
            print(f"   Name: {product.name}")
            print(f"   Brand: {product.brand}")
            print(f"   Price: {product.price}‚Ç∏")
            if product.old_price:
                print(f"   Old Price: {product.old_price}‚Ç∏")
            print(f"   Type: {product.type}")
            print(f"   Images: {len(product.image_urls)}")
        else:
            print(f"‚ùå FAILED to parse {url}")
    
    await parser.close()


if __name__ == "__main__":
    asyncio.run(test_product_parser()) 